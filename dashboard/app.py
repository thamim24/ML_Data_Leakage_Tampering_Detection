"""
Streamlit Dashboard for ML Data Leakage Detection Prototype

Interactive visualization of experimental results and explainable AI outputs.
This dashboard is intended for research demonstration purposes.
"""

import streamlit as st
import pandas as pd
import numpy as np
import os
import json
from PIL import Image
import plotly.express as px
import plotly.graph_objects as go

# Page configuration
st.set_page_config(
    page_title="Data Leakage Detection Dashboard",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Minimal CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.4rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        padding: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ---------------- Helper Functions ---------------- #

@st.cache_data
def load_data():
    """Load detection results generated by the pipeline"""
    data = {}

    if os.path.exists('outputs/anomaly_results.csv'):
        data['anomaly'] = pd.read_csv('outputs/anomaly_results.csv')

    if os.path.exists('outputs/classification_results.csv'):
        data['classification'] = pd.read_csv('outputs/classification_results.csv')

    if os.path.exists('outputs/integrity_results.csv'):
        data['integrity'] = pd.read_csv('outputs/integrity_results.csv')

    if os.path.exists('outputs/risk_summary.csv'):
        data['risk'] = pd.read_csv('outputs/risk_summary.csv')

    if os.path.exists('outputs/alerts.csv'):
        data['alerts'] = pd.read_csv('outputs/alerts.csv')
    else:
        data['alerts'] = pd.DataFrame()

    return data


def load_statistics():
    """Load summary statistics"""
    stats = {}

    if os.path.exists('outputs/anomaly_summary.json'):
        with open('outputs/anomaly_summary.json', 'r') as f:
            stats['anomaly'] = json.load(f)

    if os.path.exists('outputs/classification_metrics.json'):
        with open('outputs/classification_metrics.json', 'r') as f:
            stats['classification'] = json.load(f)

    if os.path.exists('outputs/integrity_summary.json'):
        with open('outputs/integrity_summary.json', 'r') as f:
            stats['integrity'] = json.load(f)

    if os.path.exists('outputs/risk_statistics.json'):
        with open('outputs/risk_statistics.json', 'r') as f:
            stats['risk'] = json.load(f)

    return stats

# ---------------- Pages ---------------- #

def show_home():
    """Overview page"""

    st.markdown('<div class="main-header">Data Leakage Detection Prototype</div>',
                unsafe_allow_html=True)

    st.markdown("""
    ### Multi-Modal Data Leakage Detection with Explainable AI

    This dashboard presents results from an experimental data leakage detection
    pipeline that combines:

    - Behavioral anomaly detection using Isolation Forest  
    - Document sensitivity classification  
    - File integrity verification  
    - Composite risk scoring  
    - Explainable AI using SHAP and LIME  

    The system is evaluated using synthetic enterprise data for controlled testing.
    """)

    data = load_data()
    stats = load_statistics()

    st.markdown("### Key Summary Metrics")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        if 'anomaly' in stats:
            rate = stats['anomaly'].get('anomaly_rate', 0) * 100
            st.metric(
                "Flagged Users (Unsupervised)",
                stats['anomaly'].get('anomalous_users', 0),
                f"{rate:.1f}%"
            )
        else:
            st.metric("Flagged Users (Unsupervised)", "N/A")

    with col2:
        if 'classification' in data:
            conf_docs = len(
                data['classification'][
                    data['classification']['predicted_sensitivity'] == 'confidential'
                ]
            )
            st.metric("Confidential Documents", conf_docs)
        else:
            st.metric("Confidential Documents", "N/A")

    with col3:
        if 'integrity' in stats:
            st.metric(
                "Documents with Integrity Changes",
                stats['integrity'].get('tampered_documents', 0)
            )
        else:
            st.metric("Documents with Integrity Changes", "N/A")

    with col4:
        if 'risk' in stats:
            st.metric(
                "Generated Alerts",
                stats['risk'].get('total_alerts', 0)
            )
        else:
            st.metric("Generated Alerts", "N/A")

    if 'alerts' in data and not data['alerts'].empty:
        st.markdown("### Recent Alerts")
        alerts_display = data['alerts'].head(5)[[
            'alert_id', 'entity_type', 'entity_id', 'risk_score', 'alert_priority'
        ]].copy()
        alerts_display['risk_score'] = alerts_display['risk_score'].apply(
            lambda x: f"{x:.3f}"
        )
        st.dataframe(alerts_display, use_container_width=True, hide_index=True)

    st.info(
        "All results shown are generated from controlled experimental data "
        "and are intended for research and analysis purposes."
    )


def show_behavioral_analysis():
    """Behavioral anomaly detection results"""

    st.title("Behavioral Anomaly Detection")

    data = load_data()
    if 'anomaly' not in data:
        st.error("Anomaly detection results not found.")
        return

    df = data['anomaly']

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("Total Users", len(df))

    with col2:
        flagged = df['is_anomaly'].sum()
        st.metric("Flagged Users", flagged, f"{flagged / len(df) * 100:.1f}%")

    with col3:
        critical = len(df[df['risk_level'] == 'critical'])
        st.metric("High Risk (Threshold-based)", critical)

    st.markdown("### Risk Level Distribution")
    risk_counts = df['risk_level'].value_counts().reset_index()
    risk_counts.columns = ['risk_level', 'count']

    fig = px.bar(
        risk_counts,
        x='risk_level',
        y='count',
        labels={'risk_level': 'Risk Level', 'count': 'Count'}
    )

    st.plotly_chart(fig, width="stretch")
    st.markdown("### Most Deviant User Profiles")
    st.dataframe(
        df.nsmallest(10, 'anomaly_score')[
            ['user_id', 'anomaly_score', 'risk_level']
        ],
        use_container_width=True,
        hide_index=True
    )

    st.markdown("""
    **Explainability Note**

    The SHAP visualizations below illustrate how different behavioral features
    contributed to anomaly scores under the learned baseline.
    """)

    shap_summary = 'xai_outputs/shap_summary_behavior.png'
    if os.path.exists(shap_summary):
        st.image(shap_summary, caption="SHAP Global Feature Importance",
                 use_container_width=True)
    else:
        st.info("SHAP outputs not found. Run the SHAP explainability script.")

    user_choice = st.selectbox(
        "Select a user for local explanation:",
        df.nsmallest(10, 'anomaly_score')['user_id'].tolist()
    )

    if user_choice:
        force_plot = f'xai_outputs/shap_force_user_{user_choice}.png'
        waterfall_plot = f'xai_outputs/shap_waterfall_user_{user_choice}.png'

        if os.path.exists(force_plot):
            st.image(force_plot, caption=f"SHAP Force Plot – {user_choice}")
        elif os.path.exists(waterfall_plot):
            st.image(waterfall_plot, caption=f"SHAP Waterfall Plot – {user_choice}")
        else:
            st.warning("No local explanation available for this user.")


def show_document_analysis():
    """Document classification and integrity verification"""

    st.title("Document Analysis")

    data = load_data()
    tab1, tab2 = st.tabs(["Sensitivity Classification", "Integrity Verification"])

    with tab1:
        if 'classification' not in data:
            st.error("Classification results not found.")
            return

        df = data['classification']

        col1, col2, col3 = st.columns(3)
        col1.metric("Total Documents", len(df))
        col2.metric("Confidential Documents",
                    len(df[df['predicted_sensitivity'] == 'confidential']))
        col3.metric("Mean Model Confidence", f"{df['confidence'].mean():.2f}")

        fig = px.pie(
            values=df['predicted_sensitivity'].value_counts().values,
            names=df['predicted_sensitivity'].value_counts().index
        )
        st.plotly_chart(fig, width="stretch")
        st.dataframe(df, use_container_width=True, hide_index=True)

        st.markdown("### Local Explanations (LIME)")

        doc_choice = st.selectbox(
            "Select document:",
            df['filename'].str.replace('.txt', '').tolist()
        )

        if doc_choice:
            html_path = f'xai_outputs/lime_text_explanations/{doc_choice}_explanation.html'
            if os.path.exists(html_path):
                with open(html_path, 'r', encoding='utf-8') as f:
                    st.components.v1.html(f.read(), height=400, scrolling=True)
            else:
                st.info("LIME explanation not available for this document.")

    with tab2:
        if 'integrity' not in data:
            st.error("Integrity results not found.")
            return

        df = data['integrity']

        col1, col2, col3 = st.columns(3)
        col1.metric("Files Verified", len(df))
        col2.metric("Files with Changes", df['tamper_detected'].sum())
        col3.metric("Files Intact", len(df) - df['tamper_detected'].sum())

        status_counts = df['integrity_status'].value_counts().reset_index()
        status_counts.columns = ['integrity_status', 'count']

        fig = px.bar(
            status_counts,
            x='integrity_status',
            y='count',
            labels={'integrity_status': 'Integrity Status', 'count': 'Count'}
        )

        st.plotly_chart(fig, width="stretch")
        st.dataframe(
            df[['filename', 'integrity_status', 'tamper_severity']],
            use_container_width=True,
            hide_index=True
        )


def show_risk_dashboard():
    """Composite risk scoring results"""

    st.title("Composite Risk Overview")

    data = load_data()
    if 'risk' not in data:
        st.error("Risk scoring results not found.")
        return

    df = data['risk']

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Entities Evaluated", len(df))
    col2.metric("Above Alert Threshold", len(df[df['risk_score'] >= 0.6]))
    col3.metric("Average Risk Score", f"{df['risk_score'].mean():.3f}")
    col4.metric("Maximum Risk Score", f"{df['risk_score'].max():.3f}")

    fig = px.histogram(df, x='risk_score', nbins=20)
    st.plotly_chart(fig, width="stretch")
    st.dataframe(
        df.nlargest(10, 'risk_score')[
            ['entity_type', 'entity_id', 'risk_score']
        ],
        use_container_width=True,
        hide_index=True
    )


def main():
    """Main application"""

    st.sidebar.title("Navigation")
    page = st.sidebar.radio(
        "Select View",
        ["Home", "Behavioral Analysis", "Document Analysis", "Risk Overview"]
    )

    if page == "Home":
        show_home()
    elif page == "Behavioral Analysis":
        show_behavioral_analysis()
    elif page == "Document Analysis":
        show_document_analysis()
    elif page == "Risk Overview":
        show_risk_dashboard()


if __name__ == "__main__":
    main()
